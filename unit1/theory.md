**Data Security Resiliency and Governance (offered by Computer Engineering)**  
**Code: BCE25OE02**

**UNIT 1 Introduction to data life cycle management (DLM) and Data Resiliency** 

**Data Life Cycle management (DLM):**  
Data lifecycle management (DLM) is an approach to managing data throughout its lifecycle, from data entry to data destruction

**The 3 Main Goals of Data Lifecycle Management**   
![What-is-Data-Lifecycle-Management][image1]

**Data Lifecycle Management (DLM) aims to achieve several key goals:**

* **Confidentiality:** Data Lifecycle Management (DLM) ensures that only authorized users can access and use sensitive data. This is achieved through access controls, encryption, and other security measures.

* **Integrity:** Data Lifecycle Management (DLM) practices ensure that data remains accurate, complete, and consistent throughout its lifecycle. This involves data cleansing, validation, and version control techniques.

* **Availability:** Data Lifecycle Management (DLM) helps guarantee that the right data is readily available for authorized users whenever needed. This includes measures for data backup, disaster recovery, and performance optimization.

## **Challenges to Data Lifecycle Management**

One of the main challenges facing companies is managing all of the data they create. This has become a critical issue as organizations transform into increasingly digital, data-driven enterprises. They are inundated with data on three fronts:

### **Volume of Data Sources**

### Mobile apps and devices generate massive amounts of data from new sources, such as images, audio, and video. The new data provides enterprises with opportunities to gain additional insight and value. In the past, companies were constrained by how much data they could process and store. Now they have access to unlimited storage.

### **Ubiquity of Data Locations**

Today data is everywhere — and companies need to capture it all, including customer data, financial transactions, purchase histories, customer journeys, marketing campaigns, service inquiries, market feeds, social media streams, software logs, and text messages. The more data a company can access, the more data analysis they can conduct to inform their decision making on everything from product development to marketing.

### **User Demand for Access**

Employees within an organization are increasingly using data to fulfill their team’s missions, including decision support, business outcome forecasting, and creating new solutions. The more data they can access the smarter decisions they can make.

**Stages of data life cycle \- creation, storage, usage, archival, destruction**  
As said, DLM is a process to manage the flow of data and work as a catalyst throughout the lifecycle.   
![No alt text provided for this image][image2]

1. Data Creation or Collection and gathering is the first stage in the data lifecycle when a new data value enters an organization's information system. The data gathered can be structured or unstructured. It can come in many forms i.e. PDF, images, SQL database data or word documents. During this stage, the data is not only classified by form but also by type e.g., sensitive, internal, or public. 

2. Data Storage and Maintenance: At this stage, the data is stored and maintained securely by putting in place a comprehensive data backup and recovery process. This stage ensures that the data is retained throughout its lifecycle.   
     
3. Data Usage: This third stage is one of the most important ones as organizations can view, update, process, modify and save the data stored. It entails the use of a variety of purposes to support the organization's activities, including decision-making and analysis. At this stage, to ensure that all updated data are fully traceable, an audit trail should be put in place.   
     
4. Data Sharing or Data Publication: This is the most vulnerable phase where the data is being shared with employees, stakeholders, and many other specified authorized users. The data is being shared internally or externally with customers and suppliers for various purposes such as sending monthly reports to a client. The data can be at this stage used outside of the organization and the business environment itself, and therefore the editing of incorrect data value beyond the company’s reach is difficult or sometimes impossible. 

5. Data Archiving: Archiving the data in a safe environment where the data is maintained and not frequently accessed or used. The storage of a copy is essential for litigation and investigations when needed. The DLM strategy defines at this stage when, where and for how long data should be archived. 

6. Data Deletion or re-use: knowing the costs and the challenges of data storing, data cleaning is necessary when the data is no longer accurate and useful. The deletion of the data must be done carefully and properly to ensure a good data management practice and retain the useful data only.


## **Risks of Operating Without DLM**

But organizations also face enormous challenges in ensuring that they can protect, preserve, and manage data at every stage of its lifecycle. Without a coherent and comprehensive DLM approach, organizations face huge risks, including:

### **Spiraling Storage Costs**

### The unprecedented sprawl of data across organizations, including rapid expansion and growth of unstructured data, leads to higher storage costs.

### **Security Breach Risks**

### Data security is an ever-present risk in businesses of all sizes. Any breach opens a Pandora box of problems for an organization.

### Noncompliance with Regulatory Requirements

### A growing number of new and existing regulations are placing a higher penalty on noncompliance with data privacy regulations.

### Inability to Respond to E-Discovery Demands

Expectations are high for organizations to immediately respond to electronic discovery requests. Any delay could present a risk to the organization.

### **Missed Opportunities**

The potential for missed opportunities caused by an inability to leverage the strategic value of data through business intelligence and analytics is high. Most organizations work diligently to mitigate this risk.

## 

## **Top Business Benefits of DLM**

Managing data throughout its lifecycle has become a strategic imperative for many organizations. Faced with staggering data growth, leaders recognize the need to adopt solutions and practices that not only control costs and reduce risk, but also effectively manage a wide variety of data types to achieve competitive business advantages. Companies that deploy DLM as a strategic initiative and ensure high-quality handling of the information they generate gain several business advantages.

1. **Control Costs**  
   A DLM strategy places value on data as it moves through the stages of its lifecycle. Once data is no longer useful for production environments, organizations can leverage a range of solutions to keep costs down, including backup, replication, and archiving. For example, it can be moved to less-costly storage on-premises, in the cloud, or in a hosted off-site tape vault.  
     
2. **Accelerate Time-to-Value**  
   Organizations must be able to deliver data, services, and applications faster than ever before to satisfy customers’ and employees’ demands. A DLM strategy makes everyone more productive — from customer sales reps to service technicians. This not only gives teams the ability to find and access data whenever it’s needed, but also eliminates the information silos that prevent essential business collaboration. The extraction and maintenance of information throughout the data lifecycle ensures updated values are always available.

3. **Data Security and Compliance**  
   A data security or privacy breach can be devastating to an organization. It’s not only costly, but also can result in long-term damage to a brand’s reputation and customer goodwill. Security and privacy is much more complex in today’s business environment. A DLM strategy will incorporate data protection as one of its core capabilities, ensuring a good data protection infrastructure.  
4. **Data Usability**  
   With a DLM strategy, IT can develop policies and procedures to ensure all data is consistently tagged and indexed so it can be found and accessed whenever it’s needed. Establishing enforceable governance policies future-proofs the value of data for as long as it needs to be retained. The availability of clean, useful, and precise data available to all users increases the agility and efficiency of company processes.  
5. **Improve Business Processes**  
   **Mapping** data to business processes delivers a wide range of advantages, including:  
   \-Identify and reduce process and data bottlenecks  
   \-Control redundancy through more accurate identification of duplicate records  
   \-Minimize or eliminate unwanted changes to data content  
   \-Improve consistency, reliability, and access to needed data  
   \-Improve the ability to perform root-cause analysis  
   \-Trace data lineage across the customer demographic lifecycle  
   \-Improve management of historical data

## 

## 

## **Data Lifecycle Management Stages**

To understand DLM, it’s necessary to know the phases in the data lifecycle. While there is no industry standard for enterprise DLM, most experts agree that the data lifecycle includes these six stages: creation, storage, use, sharing, archiving, and destruction.

### **Stage 1: Data Creation**

Data acquisition and capture occurs at the beginning of the cycle when an enterprise organization obtains new, vetted information, including creating data internally, purchasing third-party data, and collecting data as it streams from apps. Data is created in multiple formats, including Word documents, Excel spreadsheets, PDFs, emails, texts, images, and more.

### **Stage 2: Data Storage**

Data storage in DLM refers to implementing data redundancy and security strategies on active data (versus inactive data that is archived), as well as storing data in such a way that it can’t be accidentally altered. Data storage must be compliant with contracts and regulatory laws. That may mean only storing it on servers or only storing backups on encrypted drives.  
At this stage, the stored data is accessible only to assigned personnel. It’s common to specify the sensitivity of the data, including private, sensitive, restricted, or public. This protects the organization’s intellectual property as well as its customer relationships.  
This stage also manages data recovery plans. If a failure occurs, organizations should have a plan for continuing to access the data, such as a temporary backup on a hierarchical storage management system.

#### ***Hierarchical Storage Management (HSM)***

HSM is one type of DLM product. The hierarchy represents different types of storage media, such as RAID (redundant array of independent disks) systems, optical storage, or tape, each type represents a different level of cost and speed of retrieval when access is needed. Using an HSM product, an admin can establish guidelines for how often different kinds of files need to be copied to a backup storage device. Once the guideline has been set up, the HSM software manages everything automatically. Typically, HSM applications migrate data based on the length of time since it was last accessed, while DLM applications enable policies based on more complex criteria. Storage media for HSM includes:

### **Stage 3: Data Use**

At this stage, data usage ensures the record meets certain validations to be accessible for users. When it comes to data use, DLM defines who can use the data and for what purposes.

### **Stage 4: Data Sharing**

When data is published, it can be made available to people outside of an organization’s system, such as an invoice that uses customer data as a matter of record. Data is constantly being shared. Employees, users, customers, friends, executives, and board members either share data in the platforms they’re provided or by other methods. The more people who share data via unofficial methods, the higher the risk that it’s not compliant with governance and legal or policy regulations.

### **Stage 5: Data Archiving**

In this stage, data undergoes an archival process that ensures redundancy. Active archives are an ideal storage method for organizations that tend to store large volumes of data and who need access to the information from time to time. Archived data isn’t active. So it can be stored on drives that aren’t on the network, but accessible for ad hoc reporting and analytics. DLM strategies define when, where, and for how long data can be archived. This will vary depending on what the data is. It may be something that has legal regulations, such as personal, research, or medical data. Or it may simply be internal company documents.

### **Stage 6: Data Destruction**

In the final stage of the lifecycle, data is purged from the records and destroyed.

Risks, benefits, and best practices of Data Lifecycle Management (DLM)

Risks of operating without Data Lifecycle Management

Ignoring a comprehensive Data Lifecycle Management (DLM) plan can lead to significant risks for organizations, including: 

* Increased Security Risks: Unmanaged data can become vulnerable to cybersecurity threats, including data breaches, unauthorized access, and malware, [according to www.cio.com](https://www.cio.com/article/647027/ignoring-data-lifecycle-management-is-putting-your-business-at-risk.html).

* Data Bloat and High Storage Costs: Storing large volumes of unnecessary or duplicated datasets leads to escalating storage costs and inefficient resource utilization.

* Poor Data Quality and Inaccurate Insights: Without proper management, data can become inconsistent, outdated, or irrelevant, hindering effective data analysis and leading to inaccurate insights and poor decision-making.

* Compliance and Regulatory Risks: Failure to comply with data protection laws and industry regulations can result in fines, legal action, and reputational damage.

* Loss of Business-Critical Data: Accidental deletion or misplacement of vital data can lead to disruptions and potential business losses.

* Inefficient Data Sharing and Collaboration: Fragmented and siloed data hinders collaboration and efficient data exchange within and outside the organization. 

Benefits of implementing Data Lifecycle Management  
Adopting DLM practices offers numerous benefits, enabling organizations to maximize the value of their data assets: 

* **Improved Data Quality**: DLM processes help ensure data accuracy, reliability, and consistency throughout its lifecycle, leading to better decision-making.

* **Reduced Storage Costs**: Efficiently managing and disposing of unnecessary data reduces storage requirements and associated costs.

* **Enhanced Data Security and Privacy**: Implementing security measures, access controls, and retention policies safeguards data from unauthorized access, breaches, and misuse.

* **Regulatory Compliance**: DLM helps organizations comply with relevant data protection laws and industry regulations, avoiding penalties and legal complexities.

* **Better Data Accessibility and Usability**: Well-managed data is readily available to authorized users when needed, facilitating efficient operations and analysis.

* **Streamlined Data Management Processes**: Automating data lifecycle tasks reduces manual effort, improves efficiency, and minimizes human errors.

* **Faster and More Reliable Data Recovery**: Robust backup and recovery plans enable quick data restoration in case of emergencies or data loss.

* **Improved Business Intelligence and Decision Making**: Access to high-quality, relevant data enhances business intelligence and fosters data-driven decision-making.

* **Increased Customer Trust**: Demonstrating responsible data handling builds trust and strengthens relationships with customers and stakeholders. 

Best practices for Data Lifecycle Management  
To effectively implement DLM, organizations should focus on the following best practices: 

* Establish clear policies for data retention, access, and disposal.

* Integrate data governance to define roles and standards for data quality and security.

* Prioritize data security and privacy with measures like encryption and access controls, ensuring compliance with regulations.

* Conduct regular data quality checks and updates.

* Optimize data storage through categorization and archiving.

* Implement secure data deletion and disposal methods.

* Automate DLM processes for increased efficiency.

* Provide training on DLM policies and responsible data handling.

* Regularly review and update policies to adapt to changes.

* Establish a data recovery plan for business continuity. 

Implementing these practices helps organizations manage data effectively, reduce risks, comply with regulations, and leverage data for growth and innovation. 

## **DLM Best Practices**

## 

One of the most important responsibilities for the IT team is to develop and execute an effective DLM strategy that uses best practices to protect, preserve, and manage all of the data across the enterprise at all times. Recommended best practices include the following:

1. ### **Deploy Automated Solutions to Run DLM**

   DLM is essential to many vital business actions. To facilitate these actions and their underlying decision making, the best DLM strategies need to be repeatable, understandable, manageable, and automated. Achieving these goals is only possible when an organization deploys automated solutions that organize data into separate tiers according to specified policies, and migrate data migration from one tier to another based on those criteria.  
   

2. ### **Share the DLM Policy Across the Organization**

   Once the processes for handling data storage, management, backup, archiving, and deletion are defined, organizations need to share the process across the organization, so that everyone is aware of the new process, and stakeholders can implement the processes in a timely manner. Having clear-cut guidelines is the only way to get an entire organization onboard and participating in a new DLM program. Appointing a DLM champion can help field questions regarding the new policy and keep the organization on track with compliance and guide users.  
   

3. ### **Clearly Define Data Types**

   Organizations handle many different types of data. It’s vital to distinguish the types of data an organization manages. For example, customer data will need to be handled differently than accounting data. Identifying the type of data being stored and used is the starting point for outlining how to manage each type of data.  
   

4. ### **Implement a Robust Recovery Plan**

   Any file that lives on a physical storage device or computer is vulnerable to lose. To adequately protect data from loss and deletion, it’s essential to invest in a reliable backup system. With this solution, organizations will have the ability to retrieve any file at any time, thereby, eliminating permanent data loss.  
   

5. ### **Create a File-Naming Process**

   Losing data because it’s unsearchable is an easily preventable DLM failure. An organization should adopt a simple, yet thorough file-naming structure that allows anyone within the organization to find the data they need in seconds.  
 


### **What Is Data Resiliency?**

Data resiliency refers to the ability of an organization to protect, recover, and maintain its data in the face of unexpected events or disruptions. These events can include hardware failures, software glitches, cyberattacks, natural disasters, or human errors. A resilient data infrastructure is designed to prevent data loss, minimize downtime, and quickly recover from any adverse situations.

 **Storage technology** \- Data center End to End View  
 – overview of complete stack including storage, network, host, cluster, applications, virtual machines,   
A data center's storage technology, viewed end-to-end, encompasses the physical storage devices, the network infrastructure connecting them, the servers hosting applications, the clustering and virtualization technologies, and the applications themselves. This stack represents a complex interplay of hardware and software working together to store, access, and manage data. 

Here's a breakdown of the key components:

1\. Storage:

* **Physical Storage:**

  This includes hard disk drives (HDDs), solid-state drives (SSDs), and potentially tape storage for long-term archival. 

* **Storage Systems:**

These are the hardware platforms that house the physical storage media and provide the interfaces for accessing the data. Examples include storage area networks (SANs) and network-attached storage (NAS) devices. 

* **Storage Virtualization:**

This technology abstracts the physical storage resources, presenting them as logical pools to servers. This allows for greater flexibility and efficiency in storage management. 

* **Storage Protocols:**

Common protocols for accessing storage include Fibre Channel (FC), iSCSI, NFS, and SMB, enabling communication between servers and storage systems.   
2\. Network:

* **Connectivity:**

The network infrastructure provides the pathways for data to travel between servers, storage systems, and other components within the data center. 

* **Network Components:**

This includes switches, routers, and network interface cards (NICs) that facilitate communication. 

* **Network Virtualization:**

Technologies like VMware NSX enable network resources to be defined and managed logically in software, improving flexibility and control.   
3\. Host (Servers):

* **Compute Resources:**

Servers provide the processing power and resources needed to run applications and handle data processing tasks. 

* **Operating Systems:**

The operating system (OS) manages the server's resources and provides a platform for applications to run. 

* **Virtual Machines (VMs):**

Servers can be virtualized, allowing multiple VMs to run on a single physical server. This improves resource utilization and simplifies management.   
4\. Cluster:

* **Clustering:**

Clustering involves grouping multiple servers together to provide high availability and scalability. If one server fails, another can take over its workload. 

* **Distributed Storage:**

In some cases, storage resources can be distributed across multiple servers in a cluster, improving performance and redundancy.   
5\. Applications:

* **Software Programs:**

These are the applications that utilize the data stored and processed within the data center. 

* **Application Servers:**

Applications may run on dedicated servers or within VMs, depending on the application's requirements.   
6\. Virtual Machines (VMs):

* **Virtualization:**

VMs provide an abstraction layer between the physical hardware and the operating system and applications. 

* **Resource Allocation:**

VMs are allocated resources (CPU, memory, storage) from the underlying physical server. 

* **Management:**

VMs are managed by a hypervisor, which is a software layer that enables virtualization.   
In essence, data center storage is the foundation for all other IT operations. The storage system is connected to the network, which is in turn connected to servers that host applications. Virtualization technologies allow for efficient resource utilization, and the entire stack is managed through a combination of hardware and software components. 

**Cloud storage, virtualization, RAID levels, and storage pooling**

Cloud storage and storage virtualization are closely related concepts in modern IT infrastructure, working together to provide efficient, flexible, and scalable data storage solutions. 

Here's how they connect:

1\. **Cloud storage and storage virtualization**

* **Cloud Storage**: A service where data is stored on remote servers (cloud provider's infrastructure) accessible over the internet, allowing users to scale storage capacity as needed. Examples include Amazon S3, Google Cloud Storage, etc.

* **Storage Virtualization**: A technology that abstracts physical storage resources (like hard drives, SSDs, or storage arrays) into a unified, virtual storage pool. It acts as a software layer between applications/users and the physical storage, hiding the underlying complexities of the hardware.

* **Relationship**: Cloud providers leverage storage virtualization heavily to deliver their services effectively. They abstract vast quantities of physical storage devices into large pools, from which they provision storage to users on demand. 

2\. **Storage virtualization technologies**  
There are various ways to implement storage virtualization, each with its own advantages: 

* **Host-Based**: Virtualization software runs directly on the server (host), managing local storage resources and presenting them as virtual storage to applications. Common in hyper-converged infrastructure (HCI) and cloud storage, [according to Hewlett Packard Enterprise](https://www.hpe.com/in/en/what-is/storage-virtualization.html).

* **Array-Based**: Virtualization capabilities are embedded within the storage arrays themselves, allowing them to pool and manage storage across multiple devices within the array.

* **Network-Based**: A dedicated device (like a smart switch or server) sits between servers and storage systems, managing and abstracting storage resources across the network. This creates a single, virtual storage pool from diverse storage devices connected in a Storage Area Network (SAN). 

3\. **RAID levels**  
RAID (Redundant Array of Independent Disks) is a data storage technology that combines multiple physical drives into one or more logical units. It's considered a form of storage virtualization because it presents a unified storage device from a collection of drives. 

Here are the main RAID levels: 

* RAID 0 (Striping): Improves performance by splitting data across multiple drives, but offers no redundancy.

* RAID 1 (Mirroring): Provides data redundancy by creating exact copies of data on two or more drives.

* RAID 5 (Striping with Parity): Balances performance and redundancy by striping data across multiple drives and distributing parity information, tolerating a single drive failure.

* RAID 6 (Striping with Double Parity): Similar to RAID 5, but with an extra parity block, allowing for two simultaneous disk failures and higher fault tolerance.

* RAID 10 (Striping \+ Mirroring): Combines the mirroring of RAID 1 with the striping of RAID 0, offering high performance and redundancy, [says Lenovo](https://www.lenovo.com/in/en/glossary/what-is-computer-raid/). 

4\. Storage pooling  
Storage pooling is the aggregation of physical storage resources from multiple devices into a single, logical storage unit or pool. This pool can then be divided and allocated to different users or applications as virtual volumes or Logical Unit Numbers (LUNs).  

* Benefits: Storage pooling enhances storage utilization, simplifies management, improves performance, and enables easier scaling and data migration.

* Role in Cloud: Cloud providers use storage pooling to manage and provision storage efficiently to their customers. For instance, Google Workspace uses pooled storage, where the total storage limit is shared among users in an organization,. 

In essence, storage virtualization, RAID levels, and storage pooling are fundamental concepts and technologies that enable efficient, scalable, and resilient data storage in both on-premises and cloud environments. They allow organizations to make better use of their storage resources, simplifying management and providing flexibility to meet evolving data needs. 

storage provisioning \- Advance topics in storage virtualization – storage provisioning, thin provisioning   
Storage virtualization involves abstracting physical storage into logical storage that can be allocated to applications as needed. Storage provisioning is the process of allocating storage resources to applications, while thin provisioning is a specific method within storage provisioning that optimizes storage utilization by allocating space only as it's needed, rather than upfront. This contrasts with thick provisioning, where the full requested capacity is allocated immediately, even if it's not all used. 

Storage Provisioning:

* Involves setting up and managing storage resources for applications.

* Ensures that applications have the necessary storage capacity to operate.

* Can be implemented using various techniques, including thin and thick provisioning. 

Thin Provisioning:

* Allocates storage space to applications on demand, rather than upfront. 

* Allows for over-provisioning, where more virtual storage is allocated than physical capacity. 

* Optimizes storage utilization by only consuming space as it's used. 

* Requires monitoring to prevent the physical storage from running out. 

Thick Provisioning:

* Allocates the full requested storage capacity upfront.

* Ensures immediate availability of the requested storage.

* Can lead to underutilization of storage if applications don't use all the allocated space. 

Key Differences:

* **Storage Allocation:**

  Thin provisioning allocates space dynamically, while thick provisioning allocates it upfront. 

* **Efficiency:**

  Thin provisioning is more efficient in terms of storage utilization, while thick provisioning can lead to underutilization. 

* **Flexibility:**

  Thin provisioning is more flexible, allowing for over-provisioning and easier scaling. 

* **Cost:**

  Thin provisioning can lead to cost savings by reducing the need for upfront storage purchases. 

* **Complexity:**  
  Thin provisioning may require more monitoring to prevent over-provisioning.   
  In essence, thin provisioning is a more modern and efficient approach to storage allocation compared to the traditional thick provisioning method. It allows organizations to optimize their storage resources and potentially reduce costs, but requires careful planning and monitoring to avoid potential issues. 

  \- Cloud & Kubernetes storage stack – S3, glacier,  
  S3 Glacier is a low-cost cloud storage class within Amazon S3, optimized for long-term data archiving and backup, especially for infrequently accessed data. It offers different access tiers, including Instant Retrieval, Flexible Retrieval (formerly Glacier), and Deep Archive, each tailored for specific access patterns and cost considerations. In a Kubernetes environment, S3 Glacier can be integrated as a storage backend for persistent volumes, allowing applications to store archival data in a cost-effective manner. 

  Key aspects of S3 Glacier in a cloud and Kubernetes context:

* **Cost Optimization:**

  S3 Glacier is designed to be the most cost-effective storage class for archiving data, with options for different retrieval times and associated costs. 

* **Integration with Kubernetes:**

  Kubernetes PersistentVolumes can be configured to use S3 Glacier as a storage backend, allowing applications to leverage its low-cost storage for archival needs. 

* **Access Tiers:**

  * **S3 Glacier Instant Retrieval:** For data accessed within milliseconds, suitable for infrequently accessed data that still needs to be readily available. 

  * **S3 Glacier Flexible Retrieval:** (Formerly S3 Glacier) Offers free bulk retrieval or retrieval in minutes, ideal for backups and disaster recovery. 

  * **S3 Glacier Deep Archive:** The lowest cost storage option, suitable for long-term archival with retrieval times of 12-48 hours.   
1. **Durability and Availability:**

   S3 Glacier is designed for high durability, storing data redundantly across multiple Availability Zones, and provides high availability for data access. 

2. **Use Cases:**

   S3 Glacier is suitable for various use cases, including:

   * Long-term data archiving. 

   * Backup and disaster recovery. 

   * Data lakes and analytics. 

   * Compliance archives and digital media preservation. 

3. **Integration Methods:**  
   Kubernetes can interact with S3 Glacier through:  
   * **StorageClass:** Defining a StorageClass with S3 Glacier as the provisioner, allowing dynamic provisioning of PersistentVolumes. 

   * **Snapshot and Restore:** Using S3 as a repository for snapshot and restore functionalities within Kubernetes. 

   * **Third-party tools:** Utilizing tools that integrate with both S3 and Kubernetes to manage data transfer and lifecycle management. 

   

   

    storage tiering

   Storage tiering, also known as tiered storage, is a data management strategy that involves organizing data into different categories (tiers) based on their access frequency, cost, and performance requirements. This allows organizations to optimize storage resources by placing frequently accessed data on faster, more expensive storage and less frequently accessed data on slower, more cost-effective storage. 

   

   Here's a more detailed explanation:

   Key Concepts:

* **Tiers:**

  Storage systems are divided into tiers based on performance characteristics, such as speed and latency, and cost. 

* **Data Classification:**

  Data is categorized into different tiers based on its usage patterns and importance. For example, frequently accessed data might be placed in a high-performance tier, while infrequently accessed data might be moved to a lower-cost tier. 

* **Automated Tiering:**  
  Many modern storage systems offer automated tiering capabilities. This means that data is automatically moved between tiers based on pre-defined rules or algorithms, optimizing storage utilization without manual intervention.   
    
  Benefits of Storage Tiering: 

* **Cost Optimization:**

  By storing less frequently accessed data on cheaper storage, organizations can significantly reduce their overall storage costs. 

* **Performance Enhancement:**

  Placing frequently accessed data on high-performance storage ensures faster access times and improved application performance. 

* **Scalability:**

  Tiering allows organizations to scale their storage infrastructure more efficiently by adding capacity to lower-cost tiers as needed. 

* **Improved Resource Utilization:**

  Tiering helps to ensure that storage resources are used effectively, avoiding over-provisioning of high-performance storage. 

* **Simplified Data Management:**  
  Automated tiering simplifies storage management by dynamically adjusting data placement based on usage patterns.   
  Examples of Storage Tiers: 

* **Tier 0/Tier 1 (High-Performance):**

  Typically uses SSDs or NVMe storage for demanding workloads requiring low latency and high throughput. 

* **Tier 2 (Mid-Range):**

  May use faster HDDs or slower SSDs for less critical data with moderate performance requirements. 

* **Tier 3 (Low-Cost/Archival):**  
  Often uses slower HDDs or cloud storage for infrequently accessed data or archival purposes.   
  In essence, storage tiering is a strategic approach to data management that aims to balance performance, cost, and efficiency by intelligently placing data across different storage media based on its specific needs. 

![solid\_state\_storage-technology\_data\_by\_tier.jpg][image3]  
 **\- High Availability \- Introduction to high availability \- Clustering, failover, parallel access**   
**High availability: ensuring continuous operations**

High availability (HA) refers to the ability of a system, application, or service to remain accessible and operational for a very high percentage of the time, often aiming for “five nines” (99.999%) availability, which translates to mere minutes of downtime per year. HA is critical in environments where downtime can lead to significant financial losses, reputational damage, or safety risks. High-availability systems are designed to be resilient to failures and recover quickly after disruptions, whether planned (like maintenance) or unplanned (like hardware or software failures). 

Here's a closer look at key concepts within high availability:

1**. Clustering**

Clustering involves grouping multiple servers, called nodes, to act as a single system, providing continuous availability for critical applications or services. When one node in a cluster fails, another node seamlessly takes over the workload, minimizing downtime. 

2**. Failover**

Failover is the automatic process of switching to a backup or standby system when a primary system or component fails. This ensures that the application or service remains operational without manual intervention, [according to Fortinet](https://www.fortinet.com/resources/cyberglossary/failover). Failover mechanisms are crucial for high availability, as they enable rapid recovery from failures. 

3**. Parallel access (redundancy and load balancing)**

Parallel access, in the context of HA, leverages redundancy and load balancing to enhance availability and performance. 

* Redundancy means having duplicate hardware and/or software components (like servers, storage devices, or network paths) that can take over if one fails.

* Load balancing distributes incoming traffic or workloads across multiple servers or nodes, preventing any single component from becoming overloaded. This helps optimize resource utilization, improve performance, and eliminate single points of failure.  

Key takeaway  
High availability, through concepts like clustering, failover, and parallel access (supported by redundancy and load balancing), is essential for ensuring that systems remain operational and accessible to users with minimal disruption. It's a fundamental aspect of designing resilient and reliable IT infrastructure in today's increasingly digital world. 

**New edge technology stack: Cloud, containers, and AI applications**

The new edge technology stack combines the strengths of cloud computing, containerization, and artificial intelligence (AI) applications to deliver faster, more efficient, and secure data processing and decision-making capabilities at the network edge. 

Here's a breakdown of the key components and their synergy:

1**. Cloud computing**

* While edge computing focuses on localized data processing, cloud platforms provide the necessary infrastructure for training large-scale AI models, storing vast datasets, and managing edge devices and applications.

* Cloud-based platforms offer scalable resources like GPUs and TPUs for intensive AI model training, which are then optimized and deployed to the edge.

* Cloud integration facilitates data aggregation for deeper analysis and model updates, ensuring AI models remain relevant and accurate. 

2**. Containers**

* Containers, like Docker, provide a lightweight, portable, and isolated environment for deploying and running applications on edge devices.

* They are ideal for edge environments with limited resources, allowing for efficient execution of AI models and applications.

* Containerization simplifies development, deployment, and updates of AI applications at the edge, offering consistency across different hardware and environments.

* Container orchestration tools like Kubernetes are used to manage container deployments across thousands of edge devices, ensuring efficient resource utilization and fault tolerance. 

3\. **AI applications**

* Edge AI allows for the deployment of AI algorithms and models directly on edge devices like sensors, cameras, and industrial machines.

* This enables real-time data processing and decision-making locally, without relying on constant cloud connectivity.

* Common Edge AI applications include:

  * Autonomous vehicles for navigation and collision avoidance.

  * Predictive maintenance in smart manufacturing.

  * Real-time monitoring and security in smart cities.

  * Healthcare monitoring via wearables.

  * Personalized shopping experiences in retail. 

Key benefits

* Reduced Latency: Processing data closer to the source significantly reduces the time it takes for applications to respond, crucial for time-sensitive tasks.

* Enhanced Privacy and Security: Localized data processing minimizes the transmission of sensitive information over networks, reducing the risk of breaches and aiding in compliance with data regulations.

* Improved Reliability and Autonomy: Edge AI systems can operate independently of constant cloud connectivity, making them more resilient in remote areas or during network disruptions.

* Optimized Bandwidth and Cost Efficiency: Processing data at the edge reduces the amount of data sent to the cloud, lowering bandwidth usage and associated cloud computing costs.

* Scalability and Flexibility: Edge computing enables organizations to scale operations by adding or removing edge devices as needed, providing a flexible and adaptable architecture. 

Challenges

* Managing distributed edge infrastructure with diverse devices and platforms.

* Ensuring robust security and privacy across a wider attack surface.

* Handling data storage and management with limited resources at the edge.

* Dealing with potential resource constraints on edge devices (power, memory, processing).

* Overcoming network connectivity and reliability issues in remote or challenging environments. 

Future trends

* Increased integration with 5G networks for faster and more reliable connectivity.

* Developments in AI-powered edge hardware and accelerators.

* Advancements in federated learning for collaborative AI model training on decentralized data. 

By leveraging cloud, containers, and AI at the edge, organizations can build robust and innovative solutions that drive efficiency, deliver real-time insights, and enhance the overall user experience across a wide range of industries. 

**Disaster Recovery \- Need of disaster recovery (DR)**

The indispensable role of Disaster Recovery (DR) in modern business

In today's interconnected and increasingly digital world, businesses face a wide array of potential disruptions, ranging from natural disasters and cyberattacks to human error and system failures. A Disaster Recovery (DR) plan is a crucial component of any organization's strategy, outlining the processes, tools, and policies necessary to restore access and functionality to vital IT infrastructure and systems after a disruptive event. 

Here's why Disaster Recovery is not merely a "nice-to-have" but an absolute necessity for businesses of all sizes:

1\. Minimizing downtime and financial losses

* Unexpected downtime can be incredibly costly, impacting revenue streams, productivity, and customer satisfaction.

* DR plans, by enabling swift recovery and restoration of operations, significantly reduce the duration of outages and minimize associated financial losses.

* Estimates suggest that prolonged data loss can lead to businesses going bankrupt within a year.  

2\. Ensuring business continuity and resilience

* A well-developed DR strategy is the backbone of business continuity planning, ensuring that critical business functions can resume operations quickly, even in the face of significant challenges.

* It fosters resilience, enabling organizations to withstand disruptions and adapt to changing circumstances. 

3\. Protecting data and critical assets

* Data loss or corruption can be catastrophic for any business.

* DR solutions employ robust backup and recovery mechanisms, such as encryption and access controls, to safeguard sensitive data and ensure its integrity and confidentiality.

* Regular backups and off-site data storage are essential elements of a comprehensive DR plan. 

4\. Maintaining customer trust and reputation

* Disruptions and data breaches can severely damage customer trust and an organization's reputation.

* Prompt recovery and uninterrupted service delivery, facilitated by a strong DR plan, demonstrate reliability and preparedness, thereby preserving customer confidence and strengthening stakeholder relationships. 

5\. Ensuring regulatory compliance

* Many industries are subject to regulations and standards that mandate the implementation of DR plans and data protection measures.

* Examples include GDPR, HIPAA, SOX, and PCI-DSS, which impose requirements on how organizations handle and protect sensitive data.

* Compliance with these regulations helps organizations avoid fines, legal penalties, and reputational damage. 

6\. Enhancing security posture

* DR plans often integrate with cybersecurity strategies, incorporating measures like network segmentation, intrusion detection, and real-time monitoring to detect and mitigate threats like ransomware and malware attacks.

* This proactive approach strengthens an organization's overall security posture and helps protect against cyber threats. 

**Building blocks \- global cluster, wide-area-connector (WAC), heartbeat**

These three concepts are building blocks in the context of distributed systems, particularly those that require high availability and disaster recovery. Let's look at each one:

* **Global Cluster:** A global cluster is a system composed of multiple interconnected clusters, often geographically dispersed, that work together as a single logical unit. This architecture is essential for:

  * **Disaster Recovery**: If one cluster goes down due to a disaster (e.g., natural disaster, power outage), the other clusters can take over and continue providing services, minimizing downtime.

  * **High Availability**: By distributing the workload across multiple clusters, the system can handle more requests and maintain service even if some nodes or clusters fail.

  * **Load Balancing**: Traffic can be directed to the healthiest or least busy cluster, optimizing performance.

1. Wide-Area Connector (WAC): The wide-area connector (WAC) is a crucial component that facilitates communication and coordination between different clusters within a global cluster. It essentially acts as a bridge, enabling:

   * **Status Exchange**: WAC processes exchange information about the status of each cluster, service groups (applications and associated resources), and individual systems within the clusters.

   * **Consolidated View**: This exchange of information allows the global cluster management software (e.g., Veritas Cluster Server) to create a comprehensive picture of the entire global cluster's health and operational status.

   * **Inter-cluster Communication**: WACs enable communication and transmission of commands and results between clusters.

   * **Heartbeating**: WACs play a role in managing wide-area heartbeating to monitor the health and responsiveness of remote clusters, [according to Veritas SORT](https://sort.veritas.com/public/documents/vie/7.0/linux/productguides/html/vcs_admin/ch18s02s03s01.htm).

**Heartbeat**: In the context of global clusters, heartbeats are periodic signals or messages exchanged between nodes or clusters to monitor their health and responsiveness.

* **Node and Network Monitoring**: Heartbeats are used to confirm that individual nodes, network interfaces, and the network itself are functioning correctly.

  * **Failure Detection**: If a node or cluster fails to send heartbeat messages within a specified timeframe, the system recognizes a potential failure and can initiate failover procedures.

  * Preventing Split-brain: Heartbeats are particularly important in preventing "split-brain" scenarios in clusters, where different parts of the cluster may lose communication and mistakenly assume that other parts have failed, leading to data inconsistencies.

  * Redundancy: Heartbeats can be configured to use multiple network paths (e.g., TCP/IP and storage area networks) to ensure reliability and avoid false alarms caused by temporary network issues. 

In summary, the Wide-Area Connector (WAC) facilitates communication between clusters in a Global Cluster, utilizing Heartbeat mechanisms to monitor their health and ensure overall system availability and disaster recovery capabilities. 

**Split-brain: problem and solutions**

The term "split-brain" primarily refers to a condition where the two hemispheres of the brain are disconnected, most often due to the surgical severing of the corpus callosum. This procedure, called a corpus callosotomy, is typically performed as a last resort to treat severe and uncontrollable epilepsy when other treatments have failed. 

The problem (causes and symptoms)

* Severed Corpus Callosum: The primary cause is the surgical severing of the corpus callosum, a thick bundle of nerve fibers that connects and facilitates communication between the left and right hemispheres of the brain.

* Epileptic Seizure Spread: The corpus callosum also contributes to the spread of seizure activity from one hemisphere to the other. By severing it, the procedure aims to limit the spread of these seizures, making them less severe.

* Consequences of Disconnection: The main challenge arises from the disruption of communication between the hemispheres. Each hemisphere controls the opposite side of the body and processes information independently. This can lead to some noticeable symptoms:

  * Disconnection Syndrome: This is the most common issue, manifesting as a lack of cooperation between the two sides of the brain when performing tasks with eyes closed. The right and left sides of the body might move in conflict with each other.

  * Difficulty Transferring Information: For instance, if a split-brain patient is shown an image in their left visual field (processed by the right hemisphere) and asked to name it, they may be unable to do so verbally (as the left hemisphere is typically language dominant). However, the patient might be able to point to the correct image using their left hand (controlled by the right hemisphere).

  * Alien Hand Syndrome: In some rare cases, one hand might appear to act independently, as if it has a mind of its own.

  * Cognitive Impairments: These can include memory problems and difficulties with speech, such as stuttering or trouble finding words, although these might resolve over time. 

Solutions and management

* Corpus Callosotomy: The procedure itself is the solution to the severe epilepsy that necessitates it. While it introduces disconnection, it significantly reduces the frequency and severity of seizures, especially drop attacks, improving the patient's quality of life and decreasing the risk of injuries.

* Adaptation and Neuroplasticity: The brain possesses a remarkable ability to adapt, a phenomenon known as neuroplasticity. Over time, patients often develop compensatory strategies to mitigate the effects of the disconnection. For instance, alternative pathways or the remaining intact commissures might be utilized for some degree of interhemispheric communication.

* Rehabilitation and Therapy: Cognitive rehabilitation can be helpful in improving overall cognitive functioning, especially in cases where split-brain conditions arise from congenital issues, such as callosal agenesis. Speech therapy can address speech difficulties.

* Long-Term Follow-up: Continued monitoring and support are crucial to observe how patients adapt and to address any potential long-term effects of the surgery.

* Subcortical Pathways: Despite the severed corpus callosum, some information might still be transferred between hemispheres via subcortical structures and pathways, suggesting that the disconnection is not always complete. 

It's important to note that while the split-brain condition highlights the distinct roles of the cerebral hemispheres, the notion of people being "right-brained" or "left-brained" in everyday life has been largely challenged. Even after a corpus callosotomy, individuals often function remarkably well due to their brain's adaptability. 

**Preparing for DR – firedrill**

**Preparing for a disaster recovery (DR) fire drill: a step-by-step guide**

A disaster recovery (DR) fire drill is a crucial exercise to ensure your organization's preparedness in the face of a fire or other disruptive event that impacts your systems and data. It tests your DR plan and your team's ability to respond effectively. 

**Here's a step-by-step guide to help you prepare for a successful DR fire drill:**

1\. **Define objectives and scope**

* Clearly define what you want to achieve with the drill. Are you testing the entire DR plan, a specific component, or your team's communication protocols?

* Outline the scenario you want to simulate, the systems and processes you'll test, and the metrics you'll measure to evaluate success. 

2\. Develop a comprehensive DR plan

* Risk Assessment: Identify potential risks and vulnerabilities that could lead to a fire disaster (e.g., outdated hardware, server rooms in fire-prone areas).

* Critical Processes and Assets: Determine the business processes, applications, and data that are essential for operations and prioritize their recovery.

* Recovery Objectives: Set clear Recovery Time Objectives (RTOs) (maximum downtime) and Recovery Point Objectives (RPOs) (maximum data loss) for critical systems and data.

* **Backup Procedures**: Establish robust data backup strategies in multiple locations (e.g., on-premises, cloud, offsite) with regular backup schedules.

* **Recovery Strategies**: Outline detailed steps for recovering data, restoring systems, and communicating with stakeholders.

* **Roles and Responsibilities**: Clearly define who is responsible for each recovery task, from initiating the drill to reporting findings and managing recovery efforts.

* **Communication Protocols**: Establish clear communication channels and notification procedures for internal teams, external stakeholders, and emergency services.

* **Alternate Worksites (if applicable**): Identify backup locations where operations can continue if the primary site is compromised.

* **Documentation**: Create a comprehensive and accessible DR plan document that includes all recovery procedures, roles, contacts, and relevant information. 

3\. **Prepare for the drill**

* Communicate the Plan: Inform your team and relevant stakeholders about the drill's purpose, scope, and expectations.

* Train Your Team: Ensure all employees are familiar with the DR plan, their roles and responsibilities, evacuation routes, and how to use fire safety equipment.

* Testing Infrastructure: Verify that backup systems, communication systems, and alternate sites are ready for the drill. 

4\. **Execute the drill**

* Activate the Alarm: Initiate the fire alarm system and begin the evacuation process.

* Follow Procedures: Adhere to the established evacuation routes, assembly points, and communication protocols.

* Monitor and Observe: Designated monitors should track evacuation time, observe behavior, and identify any issues or deviations from the plan. 

5\. **Evaluate and improve**

* Post-Drill Debriefing: Conduct a meeting with key personnel to review what went well, identify weaknesses, and gather feedback from participants and observers.

* Analyze Data: Analyze data and metrics collected during the drill (e.g., evacuation time, communication effectiveness).

* Update and Refine: Use the lessons learned to update your DR plan, address any identified weaknesses, and improve future drill procedures. 

6**. Regularly schedule drills**

* Disaster recovery drills should not be a one-time event.

* Schedule regular drills (at least annually, more frequently in high-risk environments) to ensure your team remains prepared and your DR plan stays effective and up-to-date. 

By diligently following these steps and emphasizing continuous improvement, your organization can significantly enhance its disaster recovery preparedness and minimize the impact of future disruptions. 





